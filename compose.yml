version: '3.8'

networks:
  iceberg_network:
    driver: bridge

services:

  # --------------------
  # MinIO (S3-Compatible Object Storage)
  # --------------------
  minio:
    image: minio/minio:RELEASE.2024-10-13T13-34-11Z
    container_name: minio
    ports:
      - "9002:9000"   # MinIO API
      - "9001:9001"   # MinIO Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./minio_data:/data
    networks:
      iceberg_network:
        aliases:
          - minio

  # --------------------
  # Iceberg REST Catalog
  # --------------------
  iceberg_rest:
    image: tabulario/iceberg-rest:1.6.0
    container_name: iceberg_rest
    ports:
      - "8181:8181"
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
      CATALOG_WAREHOUSE: s3://practice-bucket/iceberg_warehouse
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: true
      CATALOG_S3_ACCESS__KEY__ID: minioadmin
      CATALOG_S3_SECRET__ACCESS__KEY: minioadmin
    depends_on:
      - minio
    networks:
      - iceberg_network

  # --------------------
  # DuckDB Lab (Example Environment)
  # --------------------
  duckdb_lab:
    build: .
    container_name: duckdb_lab
    volumes:
      - ./sample_data:/lab/sample_data
      - ./scripts:/lab/scripts
    depends_on:
      - minio
      - iceberg_rest
    stdin_open: true
    tty: true
    # PyIceberg configuration for the DuckDB container
    environment:
      PYICEBERG_HOME: /lab/sample_data
      PYICEBERG_CATALOG__REST__URI: http://iceberg_rest:8181/
      PYICEBERG_CATALOG__REST__WAREHOUSE: s3://practice-bucket/
      PYICEBERG_CATALOG__REST__IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      PYICEBERG_CATALOG__REST__S3__ENDPOINT: http://minio:9000
      PYICEBERG_CATALOG__REST__S3__ACCESS-KEY-ID: minioadmin
      PYICEBERG_CATALOG__REST__S3__SECRET-ACCESS-KEY: minioadmin

    networks:
      - iceberg_network

  # --------------------
  # ClickHouse Database
  # --------------------
  clickhouse-server:
    image: clickhouse/clickhouse-server
    container_name: clickhouse-server
    environment:
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DB: default
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
      CLICKHOUSE_DEFAULT_CONFIG_SETTINGS: |
        <allow_experimental_database_iceberg>1</allow_experimental_database_iceberg>
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
    #  - ./sample_data:/var/lib/clickhouse/user_files
      - ./SQL queries:/sql
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8123/ping"]
      interval: 5s
      retries: 20
    # Must be on the same network to talk to minio and iceberg_rest
    networks:
      - iceberg_network


  # --------------------
  # PostgreSQL Metadata DB
  # --------------------
  postgres:
    image: postgres:16.4
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db:/var/lib/postgresql/data
    networks:
      - iceberg_network

  # --------------------
  # Airflow Init (DB + user)
  # --------------------
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-init
    depends_on:
      - postgres
      - minio # ADDED
      - iceberg_rest # ADDED
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: "1234567890"
    env_file:
      - .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/data:/opt/airflow/data
    networks:
      - iceberg_network # ADDED to ensure it can access Iceberg components
    # ... (rest of the init block)
    # The entrypoint/command is complex, ensure all dependent services are up before it runs
    entrypoint: /bin/bash
    command:
      - -c
      - |
        chown -R airflow:root /opt/airflow/data || true       
        airflow db init
        airflow users create \
          --username airflow \
          --password airflow \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com
    restart: 'no'


  # --------------------
  # Airflow Webserver
  # --------------------
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    depends_on:
      - postgres
      - airflow-init
      - minio # ADDED
      - iceberg_rest # ADDED
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
      AIRFLOW_CONN_CLICKHOUSE_DEFAULT: http://default:@clickhouse-server:8123/default
      AIRFLOW__WEBSERVER__SECRET_KEY: "1234567890"
      PYICEBERG_CATALOG__REST__URI: http://iceberg_rest:8181/
      ICEBERG_BUCKET: "practice-bucket"
      AWS_ENDPOINT: http://minio:9000
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
    env_file:
      - .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/orders:/tmp/data/orders
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/data:/opt/airflow/data
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
      
    networks:
      - iceberg_network
    command: webserver
    restart: always

  # --------------------
  # Airflow Scheduler
  # --------------------
  airflow-scheduler:
    user: root
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    depends_on:
      - airflow-init
      - postgres
      - minio
      - iceberg_rest 
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW_CONN_CLICKHOUSE_DEFAULT: http://default:@clickhouse-server:8123/default
      AIRFLOW__WEBSERVER__SECRET_KEY: "1234567890"
      PYICEBERG_CATALOG__REST__URI: http://iceberg_rest:8181/
      ICEBERG_BUCKET: "practice-bucket"
      AWS_ENDPOINT: http://minio:9000
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
    env_file:
      - .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/data:/opt/airflow/data
      - /var/run/docker.sock:/var/run/docker.sock
  
    networks:
      - iceberg_network
    command: scheduler
    restart: always


  # --------------------
  # dbt Service
  # --------------------
  dbt:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: dbt
    volumes:
      - ./SQL queries:/sql
      - ./dbt:/dbt
    working_dir: /dbt
    tty: true
    networks:
      - iceberg_network 
      

volumes:
  postgres-db:
  minio_data:

